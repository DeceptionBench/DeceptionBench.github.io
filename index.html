<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta
        content="DeceptionBench: A benchmark for evaluating deceptive alignment behaviors in large language models."
        name="description" />
    <meta content="DeceptionBench: Detecting Deceptive Tendency in Language Models" property="og:title" />
    <meta
        content="DeceptionBench: A benchmark for evaluating deceptive alignment behaviors in large language models."
        property="og:description" />
    <meta content="https://deceptionbench.github.io/data/open_graph.png" property="og:image" />
    <meta content="DeceptionBench: Detecting Deceptive Tendency in Language Models" property="twitter:title" />
    <meta
        content="DeceptionBench: A benchmark for evaluating deceptive alignment behaviors in large language models."
        property="twitter:description" />
    <meta name="twitter:site" content="@YourTwitterHandle" />
    <meta name="twitter:creator" content="@YourTwitterHandle" />
    <meta content="https://deceptionbench.github.io/data/open_graph.png" property="twitter:image" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="icon" href="logo.png" type="image/png">
    <title>DeceptionBench: Detecting Deceptive Tendency in Language Models</title>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=YOUR-GA-ID"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'YOUR-GA-ID');
    </script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preload" as="style"
        href="https://fonts.googleapis.com/css2?family=Asap:wght@700&family=Source+Sans+3:wght@400;700&display=swap">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Asap:wght@700&family=Source+Sans+3:wght@400;700&display=swap">
    <link href="style.css" rel="stylesheet" type="text/css" />
</head>

<body>
    <div class="section">
        <div class="container">
            <div class="title-row">
                <h1 class="title">DeceptionBench: Detecting Deceptive Tendency in Language Models<h1>
            </div>
            <div class="row">
                <div class="author-col">
                    Author List
                </div>
            </div>
        </div>

        <div class="row button-row">
            <a class="link-button" href="https://arxiv.org/abs/YOUR-PAPER-ID" target="_blank" class="link-block">Paper</a>
            <a class="link-button" href="https://github.com/YourUsername/DeceptionBench" class="link-block">Code</a>
            <a class="link-button" href="https://huggingface.co/datasets/YourUsername/DeceptionBench"
                class="link-block">Dataset</a>
        </div>
        <p class="tldr">
            <b>TL;DR</b>:
            We introduce DeceptionBench, a systematic benchmark to assess LLMs' deceptive tendency by evaluating consistency between utility function and model behaviors. Growing concerns about model deception highlight a lack of systematic benchmarks, hindering responsible LLMs deployment. Our benchmark addresses this gap with 180 meticulously curated scenarios across five deception categories, providing better human agreement than LLM-as-Judge evaluations.
        </p>
        
        <img class="wide-img" src="deceptionbench_human_agreement_v2.pdf" alt="Comparison of agreement with human judgments">

        <div id="content">
            <h2 class="section-header">Overview</h2>
            <div class="paragraph">
                <p>
                    Growing concerns about model deception highlight a lack of systematic benchmarks, hindering responsible LLMs deployment. To address this gap, we introduce <b>DeceptionBench</b> to assess LLMs deceptive tendency by evaluating consistency between utility function and model's behaviors.
                </p>
                <p>
                    We consider that LLMs have inherent <i>mesa utilities</i> and design <i>outer goals</i> to implicitly incentivize deviation from these mesa utilities. DeceptionBench is meticulously developed with 180 scenarios inspired by real-world interactions, strictly adhering to a predefined typology of LLM deception in five categories.
                </p>
                <p><b>Key Contributions:</b></p>
                <ul>
                    <li>
                        <b>Systematic benchmark for deception evaluation</b>: 
                        DeceptionBench provides the first systematic approach to evaluate deceptive tendencies in LLMs across diverse scenarios.
                    </li>
                    <li>
                        <b>Novel DTR metric</b>: 
                        We propose the Deception Tendency Rate (DTR) that better aligns with human judgments compared to LLM-as-Judge evaluations.
                    </li>
                    <li>
                        <b>Comprehensive evaluation across model families</b>: 
                        We evaluate both API-based and open-source models across five distinct deception categories.
                    </li>
                </ul>
            </div>

            <h2 class="section-header">Benchmark Construction</h2>
            <div class="paragraph">
                <p>
                    DeceptionBench is meticulously developed with 180 scenarios inspired by real-world interactions, strictly adhering to a predefined typology of LLM deception in five categories:
                </p>
                <ul>
                    <li><b>Sycophancy</b>: Misrepresenting and manipulating to align with perceived user preferences.</li>
                    <li><b>Strategic Deception</b>: Violating rules or manipulating information for goal achievement.</li>
                    <li><b>Honesty Evasion</b>: Avoiding honest and truthful answers under pressure or outer goal.</li>
                    <li><b>Alignment Faking</b>: Complying in alignment while regaining when not being monitored.</li>
                    <li><b>Sandbagging</b>: Deliberately underperforming or hiding capabilities for outer goal.</li>
                </ul>
                <p>
                    We deliberately employ manual curation, which is essential to capture the nuance, plausibility, and targeted nature required for deception detection.
                </p>
            </div>

            <h2 class="section-header">Evaluation Methodology</h2>
            <div class="paragraph">
                <p>
                    Evaluating deceptive behaviors using methods like LLM-as-Judge has low agreement with human annotation. Therefore, by comparing the mesa utility with the reasoning CoT and final action generated by the model, we aim to capture the model's deception tendency in a more fine-grained manner.
                </p>
                <p>
                    Since deception arises when a model's reasoning and its mesa utility are consistent while its final answer diverges from mesa utility, we define the <b>Deception Tendency Rate (DTR)</b> as the fraction of instances exhibiting the deceptive pattern. DTR offers a more reliable deception signal than LLM-as-Judge evaluations, aligning better with human judgments.
                </p>
            </div>

            <h2 class="section-header">Model Performance on DeceptionBench</h2>
            <div class="paragraph">
                <p>
                    We evaluate both API-based and open-source models across the five deception categories in DeceptionBench. The results reveal that all evaluated models exhibit some degree of deceptive tendency, with varying performance across different deception types.
                </p>
                <p>
                    The evaluation shows interesting patterns: larger models tend to show lower deceptive tendencies in some categories, while smaller open-source models exhibit higher rates of deceptive behavior. Strategic deception and honesty evasion appear to be the most challenging categories across all model families.
                </p>
            </div>
            
            <img class="wide-img" src="deception_heatmap_v2.pdf" alt="Deception tendency heatmap of models on DeceptionBench">

            <h2 class="section-header">Human Agreement Validation</h2>
            <div class="paragraph">
                <p>
                    We validate our DTR metric against human judgments and compare it with LLM-as-Judge approaches. Our results demonstrate that DTR significantly outperforms LLM-as-Judge in terms of both human agreement rate and Phi coefficient, regardless of the choice of judge models.
                </p>
                <p>
                    This superior human alignment makes DTR a more reliable metric for detecting deceptive behaviors in language models, providing researchers and practitioners with a trustworthy evaluation framework.
                </p>
            </div>

            <br> 
            <div class="paragraph-center">For more information, please check out our paper, code, and dataset:</div>
            <div class="row button-row">
                <a class="link-button"
                    href="https://arxiv.org/abs/YOUR-PAPER-ID"
                    target="_blank" class="link-block">Paper</a>
                <a class="link-button" href="https://github.com/YourUsername/DeceptionBench"
                    class="link-block">Code</a>
                <a class="link-button" href="https://huggingface.co/datasets/YourUsername/DeceptionBench"
                    class="link-block">Dataset</a>
            </div>
        </div>
    </div>
</body>

</html> 